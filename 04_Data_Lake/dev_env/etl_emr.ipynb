{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f94a34378540868bccf46cf802da86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1589539976383_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-40-188.us-west-2.compute.internal:20888/proxy/application_1589539976383_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-47-220.us-west-2.compute.internal:8042/node/containerlogs/container_1589539976383_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027dde210cdf4d2489e33770a19df8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f6b154ace10>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a043d602624723aa61716caae879e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473b1996a2c44ed6a17b39138762a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs_bucket = 's3a://udacity-dend/log_data'\n",
    "songs_bucket = 's3a://udacity-dend/song_data'\n",
    "output_bucket = 's3a://my-data-lake-gonmeso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c819bd46424099801450377516f515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \n",
    "    # Read songs data\n",
    "    print('------ Reading song data ------')\n",
    "    song_data = spark.read.json(f'{input_data}/A/A/*/*.json')  # Subset for faster experimentation\n",
    "    \n",
    "    # Create temporal table\n",
    "    print('------ Creating temporal table \"tmp_songs\" ------')\n",
    "    song_data.createOrReplaceTempView('tmp_songs')\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    print('------ Extracting songs table columns ------')\n",
    "    songs_table = spark.sql(\n",
    "    '''\n",
    "    SELECT DISTINCT song_id, title, artist_id, year, duration\n",
    "    FROM tmp_songs\n",
    "    WHERE song_id IS NOT NULL\n",
    "    ''')\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    print('------ Writing songs tables parquets ------')\n",
    "    songs_path = f'{output_data}/songs/'\n",
    "    songs_table.write.partitionBy('year', 'artist_id').parquet(\n",
    "            songs_path,\n",
    "            mode='overwrite'\n",
    "        )\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    print('------ Extracting artists table columns ------')\n",
    "    artists_table = spark.sql(\n",
    "    '''\n",
    "    SELECT DISTINCT artist_id, artist_name AS name, artist_location AS location,\n",
    "                    artist_latitude AS latitude, artist_longitude AS longitude\n",
    "    FROM tmp_songs\n",
    "    WHERE artist_id IS NOT NULL\n",
    "    ''')\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    print('------ Writing artists tables parquets ------')\n",
    "    artists_path = f'{output_data}/artists/'\n",
    "    artists_table.write.parquet(\n",
    "            artists_path,\n",
    "            mode='overwrite'\n",
    "        )\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "\n",
    "    # Read logs files\n",
    "    print('------ Reading logs data ------')\n",
    "    events = spark.read.json(f'{logs_bucket}/*/*/*.json')\n",
    "    \n",
    "    # Cast ts to timestamp\n",
    "    print('------ Casting to timestamp ------')\n",
    "    events = events.withColumn(\"start_time\",\n",
    "        (col(\"ts\")/1000).cast(\"timestamp\"))\n",
    "\n",
    "    # Create temporal table\n",
    "    print('------ Creating temporal table \"tmp_events\" ------')\n",
    "    events.createOrReplaceTempView('tmp_events')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    print('------ Extracting artists table columns ------')\n",
    "    users_table = spark.sql('''\n",
    "    SELECT DISTINCT userId as user_id, firstName as first_name, lastName as last_name, gender, level\n",
    "    FROM tmp_events\n",
    "    WHERE userId IS NOT NULL\n",
    "    ''')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    print('------ Writing users tables parquets ------')\n",
    "    users_path = f'{output_data}/users/'\n",
    "    users_table.write.parquet(\n",
    "            users_path,\n",
    "            mode='overwrite'\n",
    "        )\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    print('------ Extracting time table columns ------')\n",
    "    time_table = spark.sql(\n",
    "    '''\n",
    "    SELECT DISTINCT start_time,\n",
    "                    EXTRACT(hour from start_time) as hour,\n",
    "                    EXTRACT(day from start_time) as day,\n",
    "                    EXTRACT(week from start_time) as week,\n",
    "                    EXTRACT(month from start_time) as month,\n",
    "                    EXTRACT(year from start_time) as year,\n",
    "                    DAYOFWEEK(start_time) AS weekday\n",
    "    FROM tmp_events\n",
    "    WHERE ts IS NOT NULL\n",
    "    ''')\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    print('------ Writing time tables parquets ------')\n",
    "    time_path = f'{output_data}/time/'\n",
    "    time_table.write.partitionBy('year', 'month').parquet(\n",
    "            time_path,\n",
    "            mode='overwrite'\n",
    "        )\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    print('------ Extracting songplays table columns ------')\n",
    "    songplays_table = spark.sql(\n",
    "    '''\n",
    "    SELECT e.start_time, e.userId AS user_id, e.level, e.sessionId AS session_id, e.location,\n",
    "           e.userAgent AS user_agent, s.song_id, s.artist_id\n",
    "    FROM tmp_events e\n",
    "    JOIN tmp_songs s ON e.artist = s.artist_name\n",
    "    AND e.song = s.title\n",
    "    WHERE e.page = 'NextSong'\n",
    "    ''')\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    print('------ Writing songplays tables parquets ------')\n",
    "    songplays_path = f'{output_data}/songplays/'\n",
    "    songplays_table.write.partitionBy('year', 'month').parquet(\n",
    "            songplays_path,\n",
    "            mode='overwrite'\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    process_song_data(spark, songs_bucket, output_bucket)    \n",
    "    process_log_data(spark, logs_bucket, output_bucket)\n",
    "    \n",
    "    print('Process finished successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a90ad0698de4808a4ebd7a225603649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc9c332e78644d18599eda35da693ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
